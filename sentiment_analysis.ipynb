{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data import Field,TabularDataset,LabelField,BucketIterator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert raw reviews data into dataframe, split to train and valid dataset then save to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4318\n",
      "1    4211\n",
      "Name: label, dtype: int64\n",
      "1    1120\n",
      "0    1013\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "filename=\"reviews_labeled.txt\"\n",
    "\n",
    "#Convert text file into dataframe\n",
    "file=open(filename,\"r\")\n",
    "df=pd.DataFrame()\n",
    "for line in file:\n",
    "    text,label=line.split('|')\n",
    "    df=df.append(pd.DataFrame({'review':text,'label':int(label)},index=[0]))\n",
    "\n",
    "df=df.reset_index()\n",
    "del df['index']\n",
    "\n",
    "#Shuffle rows in dataframe\n",
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)\n",
    "split_ratio=0.8\n",
    "train,valid=df[:int(split_ratio*len(df))],df[int(split_ratio*len(df)):]\n",
    "\n",
    "#Check if negative/positive rows are distributed equally\n",
    "print(train['label'].value_counts())\n",
    "print(valid['label'].value_counts())\n",
    "\n",
    "#Export to .csv\n",
    "train.to_csv('train.csv',index=False)\n",
    "valid.to_csv('valid.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>if the movie succeeds in instilling a wary sen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>half of it is composed of snappy patter and ps...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5542</th>\n",
       "      <td>a rare beautiful film</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7980</th>\n",
       "      <td>film aficionados cannot help but love cinema ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>plays out with a dogged and eventually winning...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>too much of the humor falls flat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7665</th>\n",
       "      <td>the premise of jason x is silly but strangely ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>to say this was done better in wilder s some l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4531</th>\n",
       "      <td>in his role of observer of the scene lawrence ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8217</th>\n",
       "      <td>the movie is amateurish but it s a minor treat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  label\n",
       "1586  if the movie succeeds in instilling a wary sen...      0\n",
       "3034  half of it is composed of snappy patter and ps...      0\n",
       "5542                             a rare beautiful film       1\n",
       "7980   film aficionados cannot help but love cinema ...      1\n",
       "7270  plays out with a dogged and eventually winning...      1\n",
       "1536                  too much of the humor falls flat       0\n",
       "7665  the premise of jason x is silly but strangely ...      1\n",
       "1931  to say this was done better in wilder s some l...      0\n",
       "4531  in his role of observer of the scene lawrence ...      0\n",
       "8217    the movie is amateurish but it s a minor treat       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10) #Show how dataframe looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load .csv into torch object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 8529\n",
      "Number of testing examples: 2133\n"
     ]
    }
   ],
   "source": [
    "TEXT=Field(tokenize='spacy',include_lengths = True)\n",
    "LABEL =LabelField(dtype=torch.float)\n",
    "\n",
    "datafield=[('review',TEXT),('label',LABEL)]\n",
    "train,valid=TabularDataset.splits(path='',train='train.csv',validation='valid.csv',format='csv',skip_header=True,\n",
    "                                  fields=datafield)\n",
    "\n",
    "print(f'Number of training examples: {len(train)}')\n",
    "print(f'Number of testing examples: {len(valid)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build vocabolary and keep only the most 10000 frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE=10_000\n",
    "TEXT.build_vocab(train, max_size = MAX_VOCAB_SIZE,\n",
    "                 vectors='glove.6B.100d',\n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator = BucketIterator.splits(\n",
    "    (train, valid), \n",
    "    batch_size =BATCH_SIZE,\n",
    "    device = device,\n",
    "    sort_key=lambda x:len(x.review),\n",
    "    sort_within_batch=False,\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build RNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))           \n",
    "        return self.fc(hidden.squeeze(0))\n",
    "    \n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10002, 100])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [ 0.0551,  0.2630,  0.7239,  ...,  0.0720, -0.8072,  0.3240],\n",
      "        [-0.2553,  0.1879, -0.0932,  ..., -0.3239,  0.4157,  0.7228],\n",
      "        [ 0.3005, -0.4397,  0.3765,  ..., -0.5349,  0.9316,  0.5556]])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "print(pretrained_embeddings.shape)\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX=TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX]=torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX]=torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select optimizer and criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, Evaluate, Accuracy measurement function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_pred =torch.round(torch.sigmoid(preds))\n",
    "    correct=(rounded_preds == y).float() \n",
    "    acc=correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss=0\n",
    "    epoch_acc=0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths = batch.review\n",
    "        predictions = model(text,text_lengths).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss=0\n",
    "    epoch_acc=0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text,text_lengths=batch.review\n",
    "            predictions=model(text,text_lengths).squeeze(1)\n",
    "            loss=criterion(predictions, batch.label)\n",
    "            acc=binary_accuracy(predictions, batch.label)\n",
    "            epoch_loss+=loss.item()\n",
    "            epoch_acc+=acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "import time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time=end_time-start_time\n",
    "    elapsed_mins=int(elapsed_time/60)\n",
    "    elapsed_secs=int(elapsed_time-(elapsed_mins*60))\n",
    "    return elapsed_mins,elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 2m 42s\n",
      "\tTrain Loss: 0.622 | Train Acc: 64.17%\n",
      "\t Val. Loss: 0.525 |  Val. Acc: 73.23%\n",
      "Epoch: 02 | Epoch Time: 2m 44s\n",
      "\tTrain Loss: 0.523 | Train Acc: 73.85%\n",
      "\t Val. Loss: 0.462 |  Val. Acc: 77.65%\n",
      "Epoch: 03 | Epoch Time: 2m 5s\n",
      "\tTrain Loss: 0.467 | Train Acc: 77.17%\n",
      "\t Val. Loss: 0.457 |  Val. Acc: 78.39%\n",
      "Epoch: 04 | Epoch Time: 2m 6s\n",
      "\tTrain Loss: 0.413 | Train Acc: 80.68%\n",
      "\t Val. Loss: 0.435 |  Val. Acc: 78.67%\n",
      "Epoch: 05 | Epoch Time: 2m 0s\n",
      "\tTrain Loss: 0.383 | Train Acc: 82.83%\n",
      "\t Val. Loss: 0.432 |  Val. Acc: 80.69%\n",
      "Epoch: 06 | Epoch Time: 1m 59s\n",
      "\tTrain Loss: 0.328 | Train Acc: 85.53%\n",
      "\t Val. Loss: 0.421 |  Val. Acc: 80.87%\n",
      "Epoch: 07 | Epoch Time: 1m 59s\n",
      "\tTrain Loss: 0.297 | Train Acc: 87.50%\n",
      "\t Val. Loss: 0.445 |  Val. Acc: 81.20%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "valid_a=[]\n",
    "train_a=[]\n",
    "valid_l=[]\n",
    "train_l=[]\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time=time.time()\n",
    "    \n",
    "    train_loss,train_acc=train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss,valid_acc=evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time=time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    valid_a.append(valid_acc)\n",
    "    train_a.append(train_acc)\n",
    "    valid_l.append(valid_loss)\n",
    "    train_l.append(train_loss)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "test_loss, test_acc = evaluate(model, valid_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize training and validation accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_a, label='Training accuracy')\n",
    "plt.plot(valid_a, label='Validation accuracy')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_l, label='Training accuracy')\n",
    "plt.plot(valid_l, label='Validation accuracy')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
