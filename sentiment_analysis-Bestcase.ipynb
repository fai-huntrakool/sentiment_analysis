{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data import Field,TabularDataset,LabelField,BucketIterator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert raw reviews data into dataframe, split to train and valid dataset then save to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    4290\n",
      "0    4239\n",
      "Name: label, dtype: int64\n",
      "0    543\n",
      "1    523\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "filename=\"reviews_labeled.txt\"\n",
    "\n",
    "#Convert text file into dataframe\n",
    "file=open(filename,\"r\")\n",
    "df=pd.DataFrame()\n",
    "for line in file:\n",
    "    text,label=line.split('|')\n",
    "    df=df.append(pd.DataFrame({'review':text,'label':int(label)},index=[0]))\n",
    "\n",
    "df=df.reset_index()\n",
    "del df['index']\n",
    "\n",
    "#Shuffle rows in dataframe\n",
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)\n",
    "split_ratio=0.8\n",
    "train,valid=df[:int(split_ratio*len(df))],df[int(split_ratio*len(df)):]\n",
    "valid,test=valid[:int(len(valid)/2)],valid[int(len(valid)/2):]\n",
    "\n",
    "#Check if negative/positive rows are distributed equally\n",
    "print(train['label'].value_counts())\n",
    "print(valid['label'].value_counts())\n",
    "\n",
    "#Export to .csv\n",
    "train.to_csv('train.csv',index=False)\n",
    "valid.to_csv('valid.csv',index=False)\n",
    "test.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 8529\n",
      "Size of validation set: 1066\n",
      "Size of test set: 1067\n"
     ]
    }
   ],
   "source": [
    "print(f'Size of training set: {len(train)}')\n",
    "print(f'Size of validation set: {len(valid)}')\n",
    "print(f'Size of test set: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3038</th>\n",
       "      <td>a lousy movie that s not merely unwatchable bu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8049</th>\n",
       "      <td>the film is impressive for the sights and soun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10490</th>\n",
       "      <td>bittersweet comedy drama full of life hand ges...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4209</th>\n",
       "      <td>often lingers just as long on the irrelevant a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7202</th>\n",
       "      <td>ryan gosling is at 22 a powerful young actor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2643</th>\n",
       "      <td>i didn t believe for a moment in these villain...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6002</th>\n",
       "      <td>the film benefits greatly from a less manic to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5069</th>\n",
       "      <td>ultimately sarah s dedication to finding her h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10379</th>\n",
       "      <td>the rare imax movie that you ll wish was longe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7329</th>\n",
       "      <td>the film delivers what it promises a look at t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  label\n",
       "3038   a lousy movie that s not merely unwatchable bu...      0\n",
       "8049   the film is impressive for the sights and soun...      1\n",
       "10490  bittersweet comedy drama full of life hand ges...      1\n",
       "4209   often lingers just as long on the irrelevant a...      0\n",
       "7202       ryan gosling is at 22 a powerful young actor       1\n",
       "2643   i didn t believe for a moment in these villain...      0\n",
       "6002   the film benefits greatly from a less manic to...      1\n",
       "5069   ultimately sarah s dedication to finding her h...      0\n",
       "10379  the rare imax movie that you ll wish was longe...      1\n",
       "7329   the film delivers what it promises a look at t...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10) #Show how dataframe looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load .csv into torch object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 8529\n",
      "Number of testing examples: 1066\n"
     ]
    }
   ],
   "source": [
    "TEXT=Field(tokenize='spacy',include_lengths = True)\n",
    "LABEL =LabelField(dtype=torch.float)\n",
    "\n",
    "datafield=[('review',TEXT),('label',LABEL)]\n",
    "train,valid=TabularDataset.splits(path='',train='train.csv',validation='valid.csv',format='csv',skip_header=True,\n",
    "                                  fields=datafield)\n",
    "test=TabularDataset(path='test.csv',format='csv',fields=datafield)\n",
    "\n",
    "print(f'Number of training examples: {len(train)}')\n",
    "print(f'Number of testing examples: {len(valid)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build vocabolary and keep only the most 10000 frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE=25_000\n",
    "TEXT.build_vocab(train, max_size = MAX_VOCAB_SIZE,\n",
    "                 vectors='glove.6B.100d',\n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator,test_iterator = BucketIterator.splits(\n",
    "    (train, valid,test), \n",
    "    batch_size =BATCH_SIZE,\n",
    "    device = device,\n",
    "    sort_key=lambda x:len(x.review),\n",
    "    sort_within_batch=False,\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build RNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))           \n",
    "        return self.fc(hidden.squeeze(0))\n",
    "    \n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16477, 100])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [ 0.0109, -0.8665,  0.2749,  ...,  0.1131,  0.1014,  0.3842],\n",
      "        [ 0.2512,  0.6499, -0.2465,  ...,  0.0659, -0.9114,  0.4129],\n",
      "        [ 1.7732,  1.7099, -0.3068,  ..., -2.1893,  3.6804, -0.7701]])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "print(pretrained_embeddings.shape)\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX=TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX]=torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX]=torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select optimizer and criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, Evaluate, Accuracy measurement function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds =torch.round(torch.sigmoid(preds))\n",
    "    correct=(rounded_preds == y).float() \n",
    "    acc=correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss=0\n",
    "    epoch_acc=0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths = batch.review\n",
    "        predictions = model(text,text_lengths).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss=0\n",
    "    epoch_acc=0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text,text_lengths=batch.review\n",
    "            predictions=model(text,text_lengths).squeeze(1)\n",
    "            loss=criterion(predictions, batch.label)\n",
    "            acc=binary_accuracy(predictions, batch.label)\n",
    "            epoch_loss+=loss.item()\n",
    "            epoch_acc+=acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "import time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time=end_time-start_time\n",
    "    elapsed_mins=int(elapsed_time/60)\n",
    "    elapsed_secs=int(elapsed_time-(elapsed_mins*60))\n",
    "    return elapsed_mins,elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 3m 41s\n",
      "\tTrain Loss: 0.614 | Train Acc: 66.05%\n",
      "\t Val. Loss: 0.527 |  Val. Acc: 73.27%\n",
      "Epoch: 02 | Epoch Time: 3m 56s\n",
      "\tTrain Loss: 0.508 | Train Acc: 74.86%\n",
      "\t Val. Loss: 0.485 |  Val. Acc: 76.23%\n",
      "Epoch: 03 | Epoch Time: 3m 56s\n",
      "\tTrain Loss: 0.424 | Train Acc: 80.42%\n",
      "\t Val. Loss: 0.485 |  Val. Acc: 78.38%\n",
      "Epoch: 04 | Epoch Time: 3m 54s\n",
      "\tTrain Loss: 0.363 | Train Acc: 84.17%\n",
      "\t Val. Loss: 0.474 |  Val. Acc: 77.15%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "valid_a=[]\n",
    "train_a=[]\n",
    "valid_l=[]\n",
    "train_l=[]\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time=time.time()\n",
    "    \n",
    "    train_loss,train_acc=train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss,valid_acc=evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time=time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    valid_a.append(valid_acc)\n",
    "    train_a.append(train_acc)\n",
    "    valid_l.append(valid_loss)\n",
    "    train_l.append(train_loss)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize training and validation accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_a, label='Training accuracy')\n",
    "plt.plot(valid_a, label='Validation accuracy')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_l, label='Training Loss')\n",
    "plt.plot(valid_l, label='Validation Loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict input sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "def predict(model_name,text):\n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(text)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    rounded_preds =torch.round(torch.sigmoid(model(tensor,length_tensor)))\n",
    "    return ('Positive' if rounded_preds.item()<=0.5 else 'Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('tut1-model.pt','it s a familiar story but one that is presented with great sympathy and intelligence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('tut1-model.pt','unfortunately the story and the actors are served with a hack script')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
